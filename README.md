# rutokenizer - инструменты для сегментации и токенизации русскоязычного текста с исправлением ошибок

Пакет содержит набор классов, которые умеют выделать предложения из текста (сегментация) и
разбивать предложения на слова (токенизация) с учетом многословных единиц текста типа "из-за" или "какой-то".

Код рассчитан на быстрое прототипирование NLP приложений, допуская простую кастомизацию алгоритма
токенизации путем модификации исходного текста. Разумеется, скорость работы данного токенизатора меньше, чем
[вариантов на C++](https://github.com/Koziev/GrammarEngine/blob/master/src/ai/some/SentenceTokenizer.cpp).

Код написан для Питона 2й и 3й ветки, работает в Windows и Linux.

## Установка

Наберите в консоли, возможно потребуется sudo:

```
pip install git+https://github.com/Koziev/rutokenizer
```

В комплект входят "батарейки" - файлы данных с правилами для многословных элементов, получаемых
из [Грамматического Словаря](https://github.com/Koziev/GrammarEngine).


## API

Для токенизации необходимо создать экземпляр класса rutokenizer.Tokenizer,
вызвать у него метод load() для загрузки словарной информации, необходимой
для правильной работы с многословными единицами. Далее нужно вызывать метод
tokenize, передавая ему юникодную строку предложения и получая список слов.


## Примеры

Токенизация предложения:


```
import rutokenizer

t = rutokenizer.Tokenizer()
t.load()
t.tokenize(u'Я-то из-за угла вышел.')

for t in t.tokenize(u'Я-то из-за угла вышел.'):
    print(u'{}'.format(t)
```

Результат работы:

```
Я
-
то
из-за
угла
вышел
.
```
