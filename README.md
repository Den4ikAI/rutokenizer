# rutokenizer - инструменты для сегментации и токенизации русскоязычного текста с исправлением ошибок

Пакет содержит набор классов, которые умеют выделать предложения из текста (сегментация) и
разбивать предложения на слова (токенизация) с учетом многословных единиц текста типа "из-за" или "какой-то".

Код написан для Питона 2й и 3й ветки.

## Установка

Наберите в консоли, возможно потребуется sudo:

```
pip install git+https://github.com/Koziev/rutokenizer
```

## API

Для токенизации необходимо создать экземпляр класса rutokenizer.Tokenizer,
вызвать у него метод load() для загрузки словарной информации, необходимой
для правильной работы с многословными единицами. Далее нужно вызывать метод
tokenize, передавая ему юникодную строку предложения и получая список слов.


## Примеры

Токенизация предложения:


```
import rutokenizer

t = rutokenizer.Tokenizer()
t.load()
t.tokenize(u'Я-то из-за угла вышел.')

for t in t.tokenize(u'Я-то из-за угла вышел.'):
    print(u'{}'.format(t)
```

Результат работы:

```
Я
-
то
из-за
угла
вышел
.
```
